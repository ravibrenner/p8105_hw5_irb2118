---
title: "P8105 - HW5"
author: "Ravi Brenner"
output: github_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(patchwork)
set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%",
  dpi = 300
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Introduction
This homework assignment will cover a few different ways to iterate through variables to write more efficient data analysis pipelines in R.

# Methods
The data for this problem are created via simulation, and from a dataset available [here](https://github.com/washingtonpost/data-homicides).

# Problems

## Problem 1
The birthday problem: suppose there are n people in a room. What is the probability that at least two people share the same birthday? We can solve this mathematically, but here we will do so with a relatively simple simulation.

First, a function to simulate a room of `n` people, with equally distributed birthdays over 365 days.
```{r}
bday_sim <- function(n) {
  bdays <- sample(1:365, size = n, replace = TRUE)

  duplicate <- length(unique(bdays)) < n

  return(duplicate)
}
```

Next, we can run this function 10,000 times for group sizes between 2 and 50:
```{r}
sim_res <-
  expand_grid(n = 2:50,
              iter = 1:10000) |>
  mutate(res = map_lgl(n, bday_sim)) |>
  group_by(n) |>
  summarize(prob = mean(res))
```

Looking at the results:
```{r}
sim_res |>
  ggplot(aes(x = n, y = prob)) +
  geom_line() +
  labs(x = "Number of people in room",
       y = "Probability at least 2 people share a birthday")
```

The probability crosses 50% around 23 people, and reaches >95% at 50 people.

## Problem 2
This problem will use simulation to explore power in a one-sample t-test.

First, we will write a brief function to draw values from a normal distribution with $n = 30$, $\sigma = 5$, $\mu = 0$, and then calculate the mean of that distribution $\hat{\mu}$. We will also perform a t-test on that sample of 30 data points, with the null hypothesis $\mu = 0$ and $\alpha = 0.05$.

```{r}
norm_samp <- function(mu, n = 30, sigma = 5){
  sim_data <- tibble(
    x = rnorm(n = n, mean = mu, sd = sigma)
    )
  
  t_test_results <- broom::tidy(t.test(sim_data, mu = 0, conf.level = 0.95))
  
  sim_data |>
    summarize(mu_hat = t_test_results |> pull(estimate),
              p_val = t_test_results |> pull(p.value))
}

norm_samp(mu = 0)
```

Now we can generate 5000 datasets from this model with $\mu = 0$, and save the results.
```{r}
sim_results_df <- 
  tibble(iter = 1:5000,
         mu = 0) |>
  mutate(results = map(mu, norm_samp)) |>
  unnest(results)
```

We can further extend this to values of $\mu$ from 1 to 6:
```{r}
sim_results_df <-
  expand_grid(iter = 1:5000, 
              mu = 0:6) |>
  mutate(results = map(mu, norm_samp)) |>
  unnest(results)
```

Now, we can look at the proportion of times the null was rejected (i.e. the power of the test) vs. the value of $\mu$:
```{r}
sim_results_df |>
  group_by(mu) |>
  summarize(iter = n(),
            rejected = sum(if_else(p_val < 0.05,TRUE,FALSE))) |>
  mutate(prob_reject = rejected / iter) |>
  ggplot(aes(x = mu, y = prob_reject, fill = mu)) +
  geom_bar(position = "dodge",stat = "identity") +
  labs(x = expression(mu),
       y = "Proportion of times null rejected") +
  theme(legend.position = "none")
```

We can see that as $\mu$ increases, the power of the test increases. This makes intuitive sense, since the true $\mu$ value is moving further from the null value of 0.

We can also look at the average estimate of $\hat{\mu}$:

```{r, warning=FALSE, message=FALSE}
all_sim_plot <- sim_results_df |>
  group_by(mu) |>
  summarize(avg_mu_hat = mean(mu_hat)) |>
  ggplot(aes(x = mu, y = avg_mu_hat)) + 
  geom_point() +
  labs(x = "True mu value",
       y = "Average mu value from simluation",
       title = "All simulations")

rej_sim_plot <- sim_results_df |>
  mutate(rejected = if_else(p_val < 0.05, TRUE, FALSE)) |>
  group_by(mu, rejected) |>
  summarize(avg_mu_hat = mean(mu_hat)) |>
  filter(rejected == TRUE) |>
  ggplot(aes(x = mu, y = avg_mu_hat)) + 
  geom_point(color = "red") +
  labs(x = "True mu value",
       y = "Average mu value from simluation",
       title = "Simulations where null was \nrejected")

all_sim_plot + rej_sim_plot
```

In all simulations, the average $\hat{\mu}$ value was close to the true value. In simulations where the null was rejected, the average $\hat{\mu}$ value was also close to the true value when $\mu$ was large, and when it was 0, but not when it took a low value (between 1-3). For $\mu= 0$ you would expect the average to be 0, since the rejection of the null could occur in either direction (higher or lower). However for $\mu$ close to (but not equal to) 0, the null hypothesis that $\mu = 0 $ will be rejected more often at higher values than lower values, leading to the skewed results seen in the plot for $\mu$ values close to 0. Once $\mu$ is sufficiently high, you reject the null so frequently that you revocer the original normal distribution.


## Problem 3
This problem uses data fram a Washington Post GitHub repo, which can be found [here](https://github.com/washingtonpost/data-homicides).

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository here. You can read their accompanying article here.

First we will import the data.
```{r}
homicide_df <- read_csv("data/homicide-data.csv") 
```

Inspecting the data, it looks like the analysts who created it already had it in a clean and tidy format. There are `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns. There are columns for report date, the victim's name and demographic information, the city, state, latitude/longitude, and the disposition (i.e. the outcome of the investigation into the homicide).

Now, we can manipulate the data to learn more about the unsolved homicides. I will create a `city_state` variable, then summarize along that variable to get total homicides and total unsolved homicides (those with disposition “Closed without arrest” or “Open/No arrest”).

```{r}
city_df <- homicide_df |>
  mutate(city_state = str_c(city,", ", state),
         unsolved = if_else(disposition == "Closed by arrest", FALSE, TRUE)) |>
  group_by(city_state) |>
  summarize(total_homicides = n(),
            total_unsolved = sum(unsolved))
```

Looking at one city, Baltimore, MD, I can use `prop.test` to estimate the proportion that are unsolved
```{r}
baltimore_prop <- city_df |>
  filter(city_state == "Baltimore, MD") |>
  mutate(result = map2(total_unsolved, total_homicides, \(x,y) prop.test(x,y)))

baltimore_prop |>
  mutate(result_tbl = map(result, broom::tidy)) |>
  unnest(result_tbl) |>
  select(city_state, total_homicides, total_unsolved, 
         estimate, conf.low, conf.high)
```

We can turn this into one continuous pipeline, and apply it to each city in the dataframe:

```{r, warning=FALSE, message=FALSE}
city_estimates <- city_df |>
  mutate(result = map2(total_unsolved, total_homicides, \(x,y) prop.test(x,y))) |>
  mutate(result_tbl = map(result, broom::tidy)) |>
  unnest(result_tbl) |>
  select(city_state, total_homicides, total_unsolved, 
         estimate, conf.low, conf.high)

head(city_estimates)
```

Now for each city, we can see the estimated proportion and 95% confidence interval for of unsolved homicides.

We can visualize this data in a plot:
```{r}
city_estimates |>
  mutate(city_state = fct_infreq(city_state, w = estimate)) |>
  ggplot(aes(y = city_state, x = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = "Proportion of unsolved homicides",
       y = "City and State") + 
  theme(axis.text.y = element_text(size = 8))
```

With the exception of Tulsa, AL (which has just one homicide in this dataset), the confidence bands are relatively narrow, and cities range in proportion of unsolved homicides from about 25% to 75%

# Conclusion

This homework showcased several ways to use `map` functions to loop through many columns of a dataset easily in only a few lines of code.